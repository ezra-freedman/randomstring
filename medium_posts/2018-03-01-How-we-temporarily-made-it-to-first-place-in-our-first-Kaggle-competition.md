---
title: How we temporarily made it to first place in our first Kaggle competition
author: Eric Silberstein
date: 2018-03-01T21:10:25.556Z
url: https://medium.com/random-string/how-we-temporarily-made-it-to-first-place-in-our-first-kaggle-competition-c43338d0c067
---

Late last year Ezra and I decided to survey some of the exciting technology developments that we couldn’t focus on while we had our heads buried building our last company. At the top of our list was deep learning. We read about the concepts and were blown away by [demonstrations](http://www.yaronhadad.com/deep-learning-most-amazing-applications/). We wanted to gain a deeper understanding. To this end, we took Jeremy Howard and Rachel Thomas’ excellent [fast.ai class](http://www.fast.ai/) which helped us understand things in detail and get into coding our own deep learning models.

Jeremy encourages students to try Kaggle competitions. Kaggle, which is owned by Google, runs machine learning competitions like the famous [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) where teams competed to beat Netflix’s existing movie recommendation algorithm. We joined a competition about predicting customer churn for KKBox, a Spotify-like music service based in Asia. (The problem felt familiar to us from our experience consuming clinical trial data to help clinical trial sites and sponsors maximize patient retention.)

The competition provided past renewal history, listening logs and demographics for millions of users. Our job was to create an algorithm to predict the probability of users churning in an upcoming time period. Entries were scored using log loss. The closer the predictions were to reality, the lower the score. For example, suppose in actuality 5% of users up for renewal churned. If we created a naive algorithm that predicted a 100% chance of churn for all users, that would yield a log loss of 33. If we created an equally naive (but better) algorithm that predicted that no users would churn, that would yield a log loss of 1.7. If we predicted that each user had a 5% chance of churning, that would equate to a log loss of 0.20. If we had a magic algorithm that correctly predicted 100% chance of churn for exactly the 5% of users that did churn and 0% for the other users, that would be a log loss of 0. But of course that’s impossible.

We started by creating a deep learning model that took into account features such as user demographics and past payment history. Kaggle lets you submit predictions up to a few times per day. It uses a portion of your predictions to calculate your score (log loss in this case) and rank you on a leaderboard. This was incredibly addictive. We were hundreds of places back in our first submission. Over the course of about a week we worked our way into the top 20 through experimenting and learning.

The more we got absorbed into the project and obsessed over the data, the more we realized that the people who prepared the competition data had been sloppy. For example, it was clear that the demographics data file was cut after the month that was held back for prediction, and so information in that file let us determine with certainty that certain users had not churned. This type of “leak from the future” would be impossible in real life (unless time machines exist), but we couldn’t ignore the information because other contestants were clearly exploiting it. In fact, some teams were probably unaware they were using leaked information since the information was just one of many inputs to their models.

We started thinking about other ways the competition organizers might have been sloppy. The submission file contained a list of all users whose probability of churn we needed to predict and submit. Normally there would be no reason to ever think about the file as an input to a prediction model. However, we decided to look for correlations between the order of users in the file and churn. Our thinking was that if the team was sloppy with queries and scripts during data preparation there could be information leaks in the file. Here’s the first graph we plotted. The horizontal axis is the row number in the submission file and the vertical axis is the probability of churn for that user according to our then top 20 model.

![](./medium_posts/images/How-we-temporarily-made-it-to-first-place-in-our-first-Kaggle-competition/0_5oP6LErJtRQIJzfY.png)

We instantly saw we were onto something. What jumps out? The increasing density of points at lower churn values makes complete sense because most users don’t churn. The horizontal streaks between 0.8 and 1.0 also can be explained by many users receiving the same or similar predictions according to our model. However, the two dense vertical regions **absolutely should not be there**. Row order of the submission file was not an input to our model and if the users in that file were in a random order, why should our model have been assigning higher churn probabilities to users in those two regions?

It looked like the submission file was not random. We wanted to do a quick test to determine if a disproportionate number of churn users were in fact in those two regions. We threw out our deep learning model and prepared a submission in which we predicted close to 100% churn for all users in the two regions and close to 0% churn for all the rest. We submitted. **Instantly we jumped to first place.** Our score was three orders of magnitude better than the previous first place score. Our score was so good that it would be absolutely impossible in real life (unless you believe artificial intelligence can perfectly predict human behavior with limited inputs). We were briefly super excited, but then upset because all of our hard work was wasted and winning the competition was about exploiting a leak and not machine learning.

![](./medium_posts/images/How-we-temporarily-made-it-to-first-place-in-our-first-Kaggle-competition/1_tx8Ab5sBVD5YzBrz9IMEZA.png)

Our submission was on a Saturday morning. We knew once other contestants saw that such a good score was possible they would figure it out. We also suspected Kaggle would need to cancel or reset the competition. We were right — by Monday morning a whole bunch of teams submitted using the same strategy. Eventually Kaggle invalidated the competition and started it again with new data. By then we had moved onto other things!